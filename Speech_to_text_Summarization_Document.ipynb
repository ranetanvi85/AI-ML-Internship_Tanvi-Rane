{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "https://drive.google.com/file/d/1JdAhm9frg6cw4Nn1--RPm0COqCR61RUz/view?usp=drive_link"
      ],
      "metadata": {
        "id": "lDqwOX_212JY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://luvvoice.com/#google_vignette"
      ],
      "metadata": {
        "id": "p--wYYCJXOrK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8:00 AM\n",
        "\"Patient report received from the mother. The patient is awake and alert, ambulating in the room with respirations that are even and unlabored. All safety precautions are in place. Care plan and MD orders were reviewed. Vital signs are within normal limits. Emergency equipment has been checked and is available.\"\n",
        "\n",
        "9:00 AM\n",
        "\"Gastrostomy button assessed, cleaned, and rotated 360 degrees. The site is dry and intact. Placement and patency confirmed, with no residual noted. Feeding administered with Peptamen Junior at 275 milliliters over one hour. Aspiration precautions in place, and monitoring will continue. Scheduled medication was administered per MD orders.\"\n",
        "\n",
        "10:00 AM\n",
        "\"Feeding completed and flushed with 15 milliliters of water. Disconnected from the patient's gastrostomy button. Head-to-toe assessment conducted. HEENT symmetrical, with moist mucous membranes, elastic skin turgor, and midline trachea. Breath sounds are clear in all lung fields, with no cough present. Heart tones clear and capillary refill less than three seconds. No edema or cyanosis noted. Bowel sounds are present in all four quadrants, with no signs of bladder distention. Muscle tone is within normal limits, and active range of motion is ambulatory.\"\n",
        "\n",
        "12:00 PM\n",
        "\"Patient remains awake, alert, ambulating, and engaging in age-appropriate activities like watching television and playing with toys. Safety precautions are continuously observed.\"\n",
        "\n",
        "1:00 PM\n",
        "\"Gastrostomy button checked for residuals, with none found. Placement and patency confirmed. Feeding administered with Peptamen Junior at 275 milliliters. Aspiration precautions remain in place.\"\n",
        "\n",
        "2:00 PM\n",
        "\"Feeding completed and flushed with 15 milliliters of water, tolerated well by the patient. Disconnected from gastrostomy button. Scheduled medication administered per MD orders.\"\n",
        "\n",
        "3:00 PM\n",
        "\"Patient is awake and alert, with no signs or symptoms of distress. Respirations are even and unlabored.\"\n",
        "\n",
        "5:00 PM\n",
        "\"Gastrostomy button assessed for placement and residuals, with none noted. Feeding administered with Peptamen Junior at 275 milliliters per hour. Scheduled medication administered per MD orders, with no adverse reactions. Aspiration precautions in place.\"\n",
        "\n",
        "6:00 PM\n",
        "\"Feeding completed and flushed with 15 milliliters of water. Disconnected from gastrostomy button. Skin care provided with no skin breakdown observed. Safety precautions remain in place.\"\n",
        "\n",
        "8:00 PM\n",
        "\"Final report completed. Vital signs remain within normal limits. Patient handed over to the parent with no new orders noted.\""
      ],
      "metadata": {
        "id": "UWl74w1rz3Z6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Summary of Patient Care Notes:\n",
        "\n",
        "Morning Assessments (8:00 AM - 12:00 PM):\n",
        "\n",
        "Patient is awake, alert, and ambulating with normal respirations.\n",
        "Vital signs are stable, and safety precautions are in place.\n",
        "Gastrostomy button assessed, cleaned, and feeding (275 mL of Peptamen Junior) administered at scheduled times.\n",
        "Head-to-toe assessment shows no distress, clear breath sounds, and normal heart tones.\n",
        "Afternoon Assessments (1:00 PM - 6:00 PM):\n",
        "\n",
        "Gastrostomy button checked, placement confirmed, and feeding administered without residuals.\n",
        "Patient continues to engage in age-appropriate activities with safety precautions observed.\n",
        "Scheduled medications given per MD orders, with no adverse reactions noted.\n",
        "Skin care provided with no breakdown observed.\n",
        "Evening Report (8:00 PM):\n",
        "\n",
        "Final report indicates vital signs remain normal.\n",
        "Patient handed over to parent with no new orders."
      ],
      "metadata": {
        "id": "f5vs0uap73J7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The patient, a child, remained awake, alert, and ambulating throughout the day with normal respirations and stable vital signs, with all safety precautions observed. Morning assessments included the gastrostomy button being assessed, cleaned, and feeding administered (275 mL of Peptamen Junior) at scheduled times, with placement and patency confirmed. A head-to-toe assessment showed clear breath sounds, normal heart tones, and no distress. Throughout the afternoon, the gastrostomy button was checked again, and feeding continued without residuals, while the patient engaged in age-appropriate activities. Scheduled medications were administered as per MD orders without adverse reactions. Skin care was provided, showing no breakdown. By the evening, the final report noted that vital signs remained within normal limits, and the patient was handed over to the parent with no new orders."
      ],
      "metadata": {
        "id": "wXVNpCpt74g9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install whisper"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "osno8kI_cKir",
        "outputId": "64cc9c0e-ff35-4017-c9dd-0d7d6b9b123a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting whisper\n",
            "  Downloading whisper-1.1.10.tar.gz (42 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/42.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from whisper) (1.16.0)\n",
            "Building wheels for collected packages: whisper\n",
            "  Building wheel for whisper (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for whisper: filename=whisper-1.1.10-py3-none-any.whl size=41120 sha256=1a50b8b58a5eb2c4a056eedee454d4593a225206606652564532c7777b5045b6\n",
            "  Stored in directory: /root/.cache/pip/wheels/aa/7c/1d/015619716e2facae6631312503baf3c3220e6a9a3508cb14b6\n",
            "Successfully built whisper\n",
            "Installing collected packages: whisper\n",
            "Successfully installed whisper-1.1.10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai-whisper"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7UxN-8Yic57B",
        "outputId": "93729c19-b7e0-4120-a79a-e6031ebcbbe0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai-whisper\n",
            "  Downloading openai-whisper-20240930.tar.gz (800 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/800.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.4/800.5 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m798.7/800.5 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m800.5/800.5 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (0.60.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (1.26.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (2.5.0+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (4.66.6)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (10.5.0)\n",
            "Collecting tiktoken (from openai-whisper)\n",
            "  Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Collecting triton>=2.0.0 (from openai-whisper)\n",
            "  Downloading triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from triton>=2.0.0->openai-whisper) (3.16.1)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->openai-whisper) (0.43.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper) (2024.9.11)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->openai-whisper) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->openai-whisper) (3.0.2)\n",
            "Downloading triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.5/209.5 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m42.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: openai-whisper\n",
            "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai-whisper: filename=openai_whisper-20240930-py3-none-any.whl size=803321 sha256=cea033cef256ad12f8b39308de328b0d2c076bafb33754eed2f7816d64273f83\n",
            "  Stored in directory: /root/.cache/pip/wheels/dd/4a/1f/d1c4bf3b9133c8168fe617ed979cab7b14fe381d059ffb9d83\n",
            "Successfully built openai-whisper\n",
            "Installing collected packages: triton, tiktoken, openai-whisper\n",
            "Successfully installed openai-whisper-20240930 tiktoken-0.8.0 triton-3.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-docx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "isgZjS8lcbWy",
        "outputId": "250758f9-cf62-4d16-e4ed-1bc02918b081"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-docx\n",
            "  Downloading python_docx-1.1.2-py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from python-docx) (5.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.9.0 in /usr/local/lib/python3.10/dist-packages (from python-docx) (4.12.2)\n",
            "Downloading python_docx-1.1.2-py3-none-any.whl (244 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/244.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━\u001b[0m \u001b[32m194.6/244.3 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.3/244.3 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: python-docx\n",
            "Successfully installed python-docx-1.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai-whisper  # Ensures the correct package is installed\n",
        "!pip uninstall whisper"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MnMdk6WvdUAw",
        "outputId": "cc52e988-1fe4-43ca-e9fc-370fd99021cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai-whisper in /usr/local/lib/python3.10/dist-packages (20240930)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (0.60.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (1.26.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (2.5.0+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (4.66.6)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (10.5.0)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (0.8.0)\n",
            "Requirement already satisfied: triton>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (3.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from triton>=2.0.0->openai-whisper) (3.16.1)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->openai-whisper) (0.43.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper) (2024.9.11)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->openai-whisper) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->openai-whisper) (3.0.2)\n",
            "\u001b[33mWARNING: Skipping whisper as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import whisper\n",
        "from docx import Document\n",
        "from transformers import pipeline"
      ],
      "metadata": {
        "id": "pZHn6-aidFbI"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#########any audio format\n",
        "!pip install pydub"
      ],
      "metadata": {
        "id": "6mxI1bby1lAC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a732f60-c9cd-4db1-9403-0ec0f15ae0b5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pydub\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Installing collected packages: pydub\n",
            "Successfully installed pydub-0.25.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Draft 1\n",
        "\n",
        "import whisper\n",
        "from docx import Document\n",
        "from transformers import pipeline\n",
        "from pydub import AudioSegment\n",
        "import os\n",
        "\n",
        "model = whisper.load_model(\"base\")\n",
        "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
        "classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
        "\n",
        "labels = [\"Vitals\", \"Pain\", \"Skin\", \"Respiratory\", \"Neuro\", \"Musculoskeletal\", \"Gastrointestinal\", \"Medication\", \"Summary\"]\n",
        "\n",
        "def convert_to_wav(audio_path):\n",
        "    file_extension = audio_path.split(\".\")[-1].lower()\n",
        "    if file_extension != \"wav\":\n",
        "        audio = AudioSegment.from_file(audio_path)\n",
        "        wav_path = \"temp_audio.wav\"\n",
        "        audio.export(wav_path, format=\"wav\")\n",
        "        return wav_path\n",
        "    return audio_path\n",
        "\n",
        "def transcribe_and_summarize(audio_path):\n",
        "    audio_path = convert_to_wav(audio_path)\n",
        "\n",
        "    result = model.transcribe(audio_path)\n",
        "    text = result[\"text\"]\n",
        "\n",
        "    summarized_text = summarizer(text, max_length=300, min_length=100, do_sample=False)[0][\"summary_text\"]\n",
        "\n",
        "    if audio_path == \"temp_audio.wav\":\n",
        "        os.remove(audio_path)\n",
        "\n",
        "    return summarized_text\n",
        "\n",
        "def structure_text(summarized_text):\n",
        "    segments = summarized_text.split(\". \")\n",
        "\n",
        "    structured_text = {label: [] for label in labels}\n",
        "    for segment in segments:\n",
        "        if segment.strip():\n",
        "            classification = classifier(segment, candidate_labels=labels)\n",
        "            label = classification[\"labels\"][0]\n",
        "            structured_text[label].append(segment)\n",
        "\n",
        "    return structured_text\n",
        "\n",
        "def add_vitals_table(doc, vitals_data):\n",
        "\n",
        "    table = doc.add_table(rows=1, cols=5)\n",
        "    hdr_cells = table.rows[0].cells\n",
        "    hdr_cells[0].text = 'Time'\n",
        "    hdr_cells[1].text = 'Blood Pressure'\n",
        "    hdr_cells[2].text = 'Temperature'\n",
        "    hdr_cells[3].text = 'Respiration'\n",
        "    hdr_cells[4].text = 'Pulse'\n",
        "\n",
        "    for row_data in vitals_data:\n",
        "        row_cells = table.add_row().cells\n",
        "        for i, item in enumerate(row_data):\n",
        "            row_cells[i].text = str(item)\n",
        "\n",
        "def create_nursing_note_doc(structured_text, output_path=\"Draft 1.docx\"):\n",
        "    doc = Document()\n",
        "    doc.add_heading(\"Nursing Note\", 0)\n",
        "\n",
        "    for section, content in structured_text.items():\n",
        "        if content:\n",
        "            doc.add_heading(section, level=1)\n",
        "\n",
        "            if section == \"Vitals\":\n",
        "                vitals_data = []\n",
        "                for line in content:\n",
        "                    if any(x in line for x in [\"AM\", \"PM\"]):\n",
        "\n",
        "                        parts = line.split(\",\")\n",
        "                        if len(parts) >= 5:\n",
        "                            time, bp, temp, resp, pulse = parts[:5]\n",
        "                            vitals_data.append((time.strip(), bp.strip(), temp.strip(), resp.strip(), pulse.strip()))\n",
        "                add_vitals_table(doc, vitals_data)\n",
        "\n",
        "            else:\n",
        "                for line in content:\n",
        "                    if section in [\"Pain\", \"Skin\", \"Respiratory\"]:\n",
        "                        if \"No\" in line:\n",
        "                            doc.add_paragraph(\"☐ \" + line)\n",
        "                        else:\n",
        "                            doc.add_paragraph(\"☑ \" + line)\n",
        "                    else:\n",
        "                        doc.add_paragraph(line)\n",
        "\n",
        "\n",
        "    doc.save(output_path)\n",
        "\n",
        "def main(audio_path):\n",
        "    summarized_text = transcribe_and_summarize(audio_path)\n",
        "    structured_text = structure_text(summarized_text)\n",
        "    create_nursing_note_doc(structured_text)\n",
        "    print(\"Refined nursing note created successfully.\")\n",
        "\n",
        "\n",
        "audio_path = \"/content/Patient_Report_Audio 1.wav\"\n",
        "main(audio_path)"
      ],
      "metadata": {
        "id": "kjWSIscB1o9f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5bfa4c2a-0b2d-463a-9681-0f93b661e23e"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/whisper/__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(fp, map_location=device)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Refined nursing note created successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#DRAFT 2\n",
        "import whisper\n",
        "from docx import Document\n",
        "from transformers import pipeline\n",
        "from pydub import AudioSegment\n",
        "import os\n",
        "\n",
        "model = whisper.load_model(\"base\")\n",
        "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
        "classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
        "\n",
        "labels = [\"Vitals\", \"Pain\", \"Skin\", \"Respiratory\", \"Neuro\", \"Musculoskeletal\", \"Gastrointestinal\", \"Medication\", \"Summary\"]\n",
        "\n",
        "def convert_to_wav(audio_path):\n",
        "    file_extension = audio_path.split(\".\")[-1].lower()\n",
        "    if file_extension != \"wav\":\n",
        "        audio = AudioSegment.from_file(audio_path)\n",
        "        wav_path = \"temp_audio.wav\"\n",
        "        audio.export(wav_path, format=\"wav\")\n",
        "        return wav_path\n",
        "    return audio_path\n",
        "\n",
        "def transcribe_and_summarize(audio_path):\n",
        "    audio_path = convert_to_wav(audio_path)\n",
        "\n",
        "    result = model.transcribe(audio_path)\n",
        "    text = result[\"text\"]\n",
        "\n",
        "    summarized_text = summarizer(text, max_length=300, min_length=100, do_sample=False)[0][\"summary_text\"]\n",
        "\n",
        "    if audio_path == \"temp_audio.wav\":\n",
        "        os.remove(audio_path)\n",
        "\n",
        "    return summarized_text\n",
        "\n",
        "def structure_text(summarized_text):\n",
        "    segments = summarized_text.split(\". \")\n",
        "\n",
        "    structured_text = {label: [] for label in labels}\n",
        "    for segment in segments:\n",
        "        if segment.strip():\n",
        "            classification = classifier(segment, candidate_labels=labels)\n",
        "            label = classification[\"labels\"][0]\n",
        "            structured_text[label].append(segment)\n",
        "\n",
        "    return structured_text\n",
        "\n",
        "def add_vitals_table(doc, vitals_data):\n",
        "    table = doc.add_table(rows=1, cols=5)\n",
        "    hdr_cells = table.rows[0].cells\n",
        "    hdr_cells[0].text = 'Time'\n",
        "    hdr_cells[1].text = 'Blood Pressure'\n",
        "    hdr_cells[2].text = 'Temperature'\n",
        "    hdr_cells[3].text = 'Respiration'\n",
        "    hdr_cells[4].text = 'Pulse'\n",
        "\n",
        "    for row_data in vitals_data:\n",
        "        row_cells = table.add_row().cells\n",
        "        for i, item in enumerate(row_data):\n",
        "            row_cells[i].text = str(item)\n",
        "\n",
        "\n",
        "def create_nursing_note_doc(structured_text, output_path=\"Draft 2.docx\"):\n",
        "    doc = Document()\n",
        "    doc.add_heading(\"Nursing Note\", 0)\n",
        "\n",
        "\n",
        "    for section, content in structured_text.items():\n",
        "        if content:\n",
        "            doc.add_heading(section, level=1)\n",
        "\n",
        "            if section == \"Vitals\":\n",
        "                vitals_data = []\n",
        "                for line in content:\n",
        "                    if any(x in line for x in [\"AM\", \"PM\"]):\n",
        "                        parts = line.split(\",\")\n",
        "                        if len(parts) >= 5:\n",
        "                            time, bp, temp, resp, pulse = parts[:5]\n",
        "                            vitals_data.append((time.strip(), bp.strip(), temp.strip(), resp.strip(), pulse.strip()))\n",
        "                add_vitals_table(doc, vitals_data)\n",
        "\n",
        "            else:\n",
        "                for line in content:\n",
        "\n",
        "                    if section in [\"Pain\", \"Skin\", \"Respiratory\"]:\n",
        "                        if \"No\" in line:\n",
        "                            doc.add_paragraph(\"☐ \" + line)\n",
        "                        else:\n",
        "                            doc.add_paragraph(\"☑ \" + line)\n",
        "                    else:\n",
        "                        doc.add_paragraph(line)\n",
        "\n",
        "    doc.save(output_path)\n",
        "\n",
        "def main(audio_path):\n",
        "    summarized_text = transcribe_and_summarize(audio_path)\n",
        "    structured_text = structure_text(summarized_text)\n",
        "    create_nursing_note_doc(structured_text)\n",
        "    print(\"Refined nursing note created successfully.\")\n",
        "\n",
        "audio_path = \"/content/Patient_Report_Audio_2.wav\"\n",
        "main(audio_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JCRC-7hiUBGf",
        "outputId": "1db47e68-aad2-42fb-c4c6-14aab4ab6452"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/whisper/__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(fp, map_location=device)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Refined nursing note created successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8:00 AM\n",
        "\"Patient awake and alert, ambulating around the room. Respirations are even and unlabored. No signs of distress observed. All safety measures in place, care plan and MD orders reviewed. Vital signs stable and within normal limits. Emergency equipment checked and ready.\"\n",
        "\n",
        "9:00 AM\n",
        "\"Feeding tube assessed and cleaned, intact without issues. Residual checks confirm no complications. Administered 250 mL of liquid feed over one hour. Aspiration precautions remain in place. Medications administered as per MD orders.\"\n",
        "\n",
        "10:00 AM\n",
        "\"Post-feeding care completed. Tube flushed with 20 mL water. Head-to-toe physical exam conducted, including clear breath sounds, normal heart rate, no signs of edema or cyanosis. Active range of motion normal. No abdominal distension, bowel sounds present in all quadrants.\"\n",
        "\n",
        "12:00 PM\n",
        "\"Patient continues to be awake and alert, engaging in leisure activities. No concerns noted during assessment. Safety precautions are consistently monitored.\"\n",
        "\n",
        "1:00 PM\n",
        "\"Feeding tube checked for residuals, none found. Another 250 mL feed administered. Placement and patency confirmed. Aspiration precautions remain.\"\n",
        "\n",
        "2:00 PM\n",
        "\"Feeding completed, flushed with 20 mL water. No issues noted. Disconnected from the gastrostomy button. Medication administered as per schedule.\"\n",
        "\n",
        "3:00 PM\n",
        "\"Patient remains alert with no distress or discomfort. Respirations unlabored, vitals stable.\"\n",
        "\n",
        "5:00 PM\n",
        "\"Gastrostomy tube checked, residuals clear. Administered 250 mL of feed. Medications provided without adverse reactions. Aspiration precautions upheld.\"\n",
        "\n",
        "6:00 PM\n",
        "\"Feeding completed and tube flushed. Disconnected from gastrostomy tube. Skin and hygiene care completed with no issues observed. Safety precautions continue to be observed.\"\n",
        "\n",
        "8:00 PM\n",
        "\"Final assessment completed. Vitals stable and within normal limits. No new orders noted. Patient handed over to family with appropriate care instructions.\""
      ],
      "metadata": {
        "id": "GZCOdDaCaUno"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#DRAFT 3\n",
        "import whisper\n",
        "from docx import Document\n",
        "from transformers import pipeline\n",
        "from pydub import AudioSegment\n",
        "import os\n",
        "\n",
        "model = whisper.load_model(\"base\")\n",
        "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
        "classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
        "\n",
        "labels = [\"Vitals\", \"Pain\", \"Skin\", \"Respiratory\", \"Neuro\", \"Musculoskeletal\", \"Gastrointestinal\", \"Medication\", \"Summary\"]\n",
        "\n",
        "def convert_to_wav(audio_path):\n",
        "    file_extension = audio_path.split(\".\")[-1].lower()\n",
        "    if file_extension != \"wav\":\n",
        "        audio = AudioSegment.from_file(audio_path)\n",
        "        wav_path = \"temp_audio.wav\"\n",
        "        audio.export(wav_path, format=\"wav\")\n",
        "        return wav_path\n",
        "    return audio_path\n",
        "\n",
        "def transcribe_and_summarize(audio_path):\n",
        "    audio_path = convert_to_wav(audio_path)\n",
        "\n",
        "    result = model.transcribe(audio_path)\n",
        "    text = result[\"text\"]\n",
        "\n",
        "    summarized_text = summarizer(text, max_length=300, min_length=100, do_sample=False)[0][\"summary_text\"]\n",
        "\n",
        "    if audio_path == \"temp_audio.wav\":\n",
        "        os.remove(audio_path)\n",
        "\n",
        "    return summarized_text\n",
        "\n",
        "def structure_text(summarized_text):\n",
        "    segments = summarized_text.split(\". \")\n",
        "\n",
        "    structured_text = {label: [] for label in labels}\n",
        "    for segment in segments:\n",
        "        if segment.strip():\n",
        "            classification = classifier(segment, candidate_labels=labels)\n",
        "            label = classification[\"labels\"][0]\n",
        "            structured_text[label].append(segment)\n",
        "\n",
        "    return structured_text\n",
        "\n",
        "def extract_vitals(text):\n",
        "    vitals_data = []\n",
        "    vitals_keywords = [\"Blood Pressure\", \"Temperature\", \"Respiration\", \"Pulse\"]\n",
        "\n",
        "    lines = text.split(\"\\n\")\n",
        "    for line in lines:\n",
        "        if any(keyword in line for keyword in vitals_keywords):\n",
        "            parts = line.split(\",\")\n",
        "            if len(parts) >= 5:\n",
        "                time, bp, temp, resp, pulse = parts[:5]\n",
        "                vitals_data.append((time.strip(), bp.strip(), temp.strip(), resp.strip(), pulse.strip()))\n",
        "    return vitals_data\n",
        "\n",
        "\n",
        "def add_vitals_table(doc, vitals_data):\n",
        "    if vitals_data:\n",
        "        table = doc.add_table(rows=1, cols=5)\n",
        "        hdr_cells = table.rows[0].cells\n",
        "        hdr_cells[0].text = 'Time'\n",
        "        hdr_cells[1].text = 'Blood Pressure'\n",
        "        hdr_cells[2].text = 'Temperature'\n",
        "        hdr_cells[3].text = 'Respiration'\n",
        "        hdr_cells[4].text = 'Pulse'\n",
        "\n",
        "        for row_data in vitals_data:\n",
        "            row_cells = table.add_row().cells\n",
        "            for i, item in enumerate(row_data):\n",
        "                row_cells[i].text = str(item)\n",
        "\n",
        "\n",
        "def create_nursing_note_doc(structured_text, vitals_data, output_path=\"Draft 3.docx\"):\n",
        "    doc = Document()\n",
        "    doc.add_heading(\"Nursing Note\", 0)\n",
        "\n",
        "\n",
        "    for section, content in structured_text.items():\n",
        "        if content:\n",
        "            doc.add_heading(section, level=1)\n",
        "\n",
        "\n",
        "            if section == \"Vitals\" and vitals_data:\n",
        "                add_vitals_table(doc, vitals_data)\n",
        "\n",
        "            else:\n",
        "                for line in content:\n",
        "\n",
        "                    if section in [\"Pain\", \"Skin\", \"Respiratory\"]:\n",
        "                        if \"No\" in line:\n",
        "                            doc.add_paragraph(\"☐ \" + line)\n",
        "                        else:\n",
        "                            doc.add_paragraph(\"☑ \" + line)\n",
        "                    else:\n",
        "                        doc.add_paragraph(line)\n",
        "\n",
        "    doc.save(output_path)\n",
        "\n",
        "def main(audio_path):\n",
        "    summarized_text = transcribe_and_summarize(audio_path)\n",
        "\n",
        "    vitals_data = extract_vitals(summarized_text)\n",
        "\n",
        "    structured_text = structure_text(summarized_text)\n",
        "\n",
        "    create_nursing_note_doc(structured_text, vitals_data)\n",
        "    print(\"Refined nursing note created successfully.\")\n",
        "\n",
        "audio_path = \"/content/Patient_Report_Audio_3.wav\"\n",
        "main(audio_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fGXX6YQlZjT1",
        "outputId": "0cb9a68e-3edc-4075-b8c8-95d95e0bf72f"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Refined nursing note created successfully.\n"
          ]
        }
      ]
    }
  ]
}