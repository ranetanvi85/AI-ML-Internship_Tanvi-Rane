{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "cell_execution_strategy": "setup"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install whisper\n",
        "!pip install jiwer\n",
        "!pip install torch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_x2xxgzuMt8N",
        "outputId": "a6736bef-707b-46ab-9225-ffd1cc7add5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: whisper in /usr/local/lib/python3.10/dist-packages (1.1.10)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from whisper) (1.16.0)\n",
            "Requirement already satisfied: jiwer in /usr/local/lib/python3.10/dist-packages (3.0.4)\n",
            "Requirement already satisfied: click<9.0.0,>=8.1.3 in /usr/local/lib/python3.10/dist-packages (from jiwer) (8.1.7)\n",
            "Requirement already satisfied: rapidfuzz<4,>=3 in /usr/local/lib/python3.10/dist-packages (from jiwer) (3.10.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.10/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.10/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.10/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.10/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.10/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.10/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.10/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import whisper\n",
        "import torch\n",
        "from jiwer import wer, cer, mer, compute_measures\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "import time\n"
      ],
      "metadata": {
        "id": "qrcDcQIEMt_6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade --force-reinstall openai-whisper"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "2VjiSI0tNTik",
        "outputId": "e3b0f80f-8c7e-43d2-c567-53ae2c5f9569"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai-whisper\n",
            "  Using cached openai_whisper-20240930-py3-none-any.whl\n",
            "Collecting numba (from openai-whisper)\n",
            "  Using cached numba-0.60.0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.7 kB)\n",
            "Collecting numpy (from openai-whisper)\n",
            "  Using cached numpy-2.1.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "Collecting torch (from openai-whisper)\n",
            "  Using cached torch-2.5.0-cp310-cp310-manylinux1_x86_64.whl.metadata (28 kB)\n",
            "Collecting tqdm (from openai-whisper)\n",
            "  Using cached tqdm-4.66.5-py3-none-any.whl.metadata (57 kB)\n",
            "Collecting more-itertools (from openai-whisper)\n",
            "  Using cached more_itertools-10.5.0-py3-none-any.whl.metadata (36 kB)\n",
            "Collecting tiktoken (from openai-whisper)\n",
            "  Using cached tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Collecting triton>=2.0.0 (from openai-whisper)\n",
            "  Using cached triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.3 kB)\n",
            "Collecting filelock (from triton>=2.0.0->openai-whisper)\n",
            "  Using cached filelock-3.16.1-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting llvmlite<0.44,>=0.43.0dev0 (from numba->openai-whisper)\n",
            "  Using cached llvmlite-0.43.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.8 kB)\n",
            "Collecting numpy (from openai-whisper)\n",
            "  Using cached numpy-2.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "Collecting regex>=2022.1.18 (from tiktoken->openai-whisper)\n",
            "  Using cached regex-2024.9.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
            "Collecting requests>=2.26.0 (from tiktoken->openai-whisper)\n",
            "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting typing-extensions>=4.8.0 (from torch->openai-whisper)\n",
            "  Using cached typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting networkx (from torch->openai-whisper)\n",
            "  Using cached networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting jinja2 (from torch->openai-whisper)\n",
            "  Using cached jinja2-3.1.4-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting fsspec (from torch->openai-whisper)\n",
            "  Using cached fsspec-2024.10.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch->openai-whisper)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch->openai-whisper)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch->openai-whisper)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch->openai-whisper)\n",
            "  Using cached nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch->openai-whisper)\n",
            "  Using cached nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch->openai-whisper)\n",
            "  Using cached nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch->openai-whisper)\n",
            "  Using cached nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch->openai-whisper)\n",
            "  Using cached nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch->openai-whisper)\n",
            "  Using cached nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.21.5 (from torch->openai-whisper)\n",
            "  Using cached nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.4.127 (from torch->openai-whisper)\n",
            "  Using cached nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch->openai-whisper)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting sympy==1.13.1 (from torch->openai-whisper)\n",
            "  Using cached sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting mpmath<1.4,>=1.1.0 (from sympy==1.13.1->torch->openai-whisper)\n",
            "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
            "Collecting charset-normalizer<4,>=2 (from requests>=2.26.0->tiktoken->openai-whisper)\n",
            "  Using cached charset_normalizer-3.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (34 kB)\n",
            "Collecting idna<4,>=2.5 (from requests>=2.26.0->tiktoken->openai-whisper)\n",
            "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting urllib3<3,>=1.21.1 (from requests>=2.26.0->tiktoken->openai-whisper)\n",
            "  Using cached urllib3-2.2.3-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting certifi>=2017.4.17 (from requests>=2.26.0->tiktoken->openai-whisper)\n",
            "  Using cached certifi-2024.8.30-py3-none-any.whl.metadata (2.2 kB)\n",
            "Collecting MarkupSafe>=2.0 (from jinja2->torch->openai-whisper)\n",
            "  Using cached MarkupSafe-3.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n",
            "Using cached triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.5 MB)\n",
            "Using cached more_itertools-10.5.0-py3-none-any.whl (60 kB)\n",
            "Using cached numba-0.60.0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.7 MB)\n",
            "Using cached numpy-2.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (19.5 MB)\n",
            "Using cached tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "Using cached torch-2.5.0-cp310-cp310-manylinux1_x86_64.whl (906.4 MB)\n",
            "Using cached nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "Using cached nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "Using cached nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "Using cached nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "Using cached nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "Using cached nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "Using cached nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "Using cached nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "Using cached nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "Using cached nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
            "Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Using cached nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (99 kB)\n",
            "Using cached sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
            "Using cached tqdm-4.66.5-py3-none-any.whl (78 kB)\n",
            "Using cached llvmlite-0.43.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (43.9 MB)\n",
            "Using cached regex-2024.9.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (782 kB)\n",
            "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
            "Using cached typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
            "Using cached filelock-3.16.1-py3-none-any.whl (16 kB)\n",
            "Using cached fsspec-2024.10.0-py3-none-any.whl (179 kB)\n",
            "Using cached jinja2-3.1.4-py3-none-any.whl (133 kB)\n",
            "Using cached networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
            "Using cached certifi-2024.8.30-py3-none-any.whl (167 kB)\n",
            "Using cached charset_normalizer-3.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n",
            "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
            "Using cached MarkupSafe-3.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (20 kB)\n",
            "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
            "Using cached urllib3-2.2.3-py3-none-any.whl (126 kB)\n",
            "Installing collected packages: mpmath, urllib3, typing-extensions, tqdm, sympy, regex, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, networkx, more-itertools, MarkupSafe, llvmlite, idna, fsspec, filelock, charset-normalizer, certifi, triton, requests, nvidia-cusparse-cu12, nvidia-cudnn-cu12, numba, jinja2, tiktoken, nvidia-cusolver-cu12, torch, openai-whisper\n",
            "  Attempting uninstall: mpmath\n",
            "    Found existing installation: mpmath 1.3.0\n",
            "    Uninstalling mpmath-1.3.0:\n",
            "      Successfully uninstalled mpmath-1.3.0\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.2.3\n",
            "    Uninstalling urllib3-2.2.3:\n",
            "      Successfully uninstalled urllib3-2.2.3\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.12.2\n",
            "    Uninstalling typing_extensions-4.12.2:\n",
            "      Successfully uninstalled typing_extensions-4.12.2\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.66.5\n",
            "    Uninstalling tqdm-4.66.5:\n",
            "      Successfully uninstalled tqdm-4.66.5\n",
            "  Attempting uninstall: sympy\n",
            "    Found existing installation: sympy 1.13.1\n",
            "    Uninstalling sympy-1.13.1:\n",
            "      Successfully uninstalled sympy-1.13.1\n",
            "  Attempting uninstall: regex\n",
            "    Found existing installation: regex 2024.9.11\n",
            "    Uninstalling regex-2024.9.11:\n",
            "      Successfully uninstalled regex-2024.9.11\n",
            "  Attempting uninstall: nvidia-nvtx-cu12\n",
            "    Found existing installation: nvidia-nvtx-cu12 12.4.127\n",
            "    Uninstalling nvidia-nvtx-cu12-12.4.127:\n",
            "      Successfully uninstalled nvidia-nvtx-cu12-12.4.127\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.4.127\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.4.127:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.4.127\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.21.5\n",
            "    Uninstalling nvidia-nccl-cu12-2.21.5:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.21.5\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.5.147\n",
            "    Uninstalling nvidia-curand-cu12-10.3.5.147:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.5.147\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.1.3\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.1.3:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.1.3\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.4.127\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.4.127:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.4.127\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.4.127\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.4.127:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.4.127\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.4.127\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.4.127:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.4.127\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.4.5.8\n",
            "    Uninstalling nvidia-cublas-cu12-12.4.5.8:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.4.5.8\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: networkx\n",
            "    Found existing installation: networkx 3.4.2\n",
            "    Uninstalling networkx-3.4.2:\n",
            "      Successfully uninstalled networkx-3.4.2\n",
            "  Attempting uninstall: more-itertools\n",
            "    Found existing installation: more-itertools 10.5.0\n",
            "    Uninstalling more-itertools-10.5.0:\n",
            "      Successfully uninstalled more-itertools-10.5.0\n",
            "  Attempting uninstall: MarkupSafe\n",
            "    Found existing installation: MarkupSafe 3.0.2\n",
            "    Uninstalling MarkupSafe-3.0.2:\n",
            "      Successfully uninstalled MarkupSafe-3.0.2\n",
            "  Attempting uninstall: llvmlite\n",
            "    Found existing installation: llvmlite 0.43.0\n",
            "    Uninstalling llvmlite-0.43.0:\n",
            "      Successfully uninstalled llvmlite-0.43.0\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.10\n",
            "    Uninstalling idna-3.10:\n",
            "      Successfully uninstalled idna-3.10\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.10.0\n",
            "    Uninstalling fsspec-2024.10.0:\n",
            "      Successfully uninstalled fsspec-2024.10.0\n",
            "  Attempting uninstall: filelock\n",
            "    Found existing installation: filelock 3.16.1\n",
            "    Uninstalling filelock-3.16.1:\n",
            "      Successfully uninstalled filelock-3.16.1\n",
            "  Attempting uninstall: charset-normalizer\n",
            "    Found existing installation: charset-normalizer 3.4.0\n",
            "    Uninstalling charset-normalizer-3.4.0:\n",
            "      Successfully uninstalled charset-normalizer-3.4.0\n",
            "  Attempting uninstall: certifi\n",
            "    Found existing installation: certifi 2024.8.30\n",
            "    Uninstalling certifi-2024.8.30:\n",
            "      Successfully uninstalled certifi-2024.8.30\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.1.0\n",
            "    Uninstalling triton-3.1.0:\n",
            "      Successfully uninstalled triton-3.1.0\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.32.3\n",
            "    Uninstalling requests-2.32.3:\n",
            "      Successfully uninstalled requests-2.32.3\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.3.1.170\n",
            "    Uninstalling nvidia-cusparse-cu12-12.3.1.170:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.3.1.170\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.1.0.70\n",
            "    Uninstalling nvidia-cudnn-cu12-9.1.0.70:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.1.0.70\n",
            "  Attempting uninstall: numba\n",
            "    Found existing installation: numba 0.60.0\n",
            "    Uninstalling numba-0.60.0:\n",
            "      Successfully uninstalled numba-0.60.0\n",
            "  Attempting uninstall: jinja2\n",
            "    Found existing installation: Jinja2 3.1.4\n",
            "    Uninstalling Jinja2-3.1.4:\n",
            "      Successfully uninstalled Jinja2-3.1.4\n",
            "  Attempting uninstall: tiktoken\n",
            "    Found existing installation: tiktoken 0.8.0\n",
            "    Uninstalling tiktoken-0.8.0:\n",
            "      Successfully uninstalled tiktoken-0.8.0\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.1.9\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.1.9:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.1.9\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.5.0\n",
            "    Uninstalling torch-2.5.0:\n",
            "      Successfully uninstalled torch-2.5.0\n",
            "  Attempting uninstall: openai-whisper\n",
            "    Found existing installation: openai-whisper 20240930\n",
            "    Uninstalling openai-whisper-20240930:\n",
            "      Successfully uninstalled openai-whisper-20240930\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cupy-cuda12x 12.2.0 requires numpy<1.27,>=1.20, but you have numpy 2.0.2 which is incompatible.\n",
            "gcsfs 2024.6.1 requires fsspec==2024.6.1, but you have fsspec 2024.10.0 which is incompatible.\n",
            "gensim 4.3.3 requires numpy<2.0,>=1.18.5, but you have numpy 2.0.2 which is incompatible.\n",
            "pytensor 2.25.5 requires numpy<2,>=1.17.0, but you have numpy 2.0.2 which is incompatible.\n",
            "tensorflow 2.17.0 requires numpy<2.0.0,>=1.23.5; python_version <= \"3.11\", but you have numpy 2.0.2 which is incompatible.\n",
            "thinc 8.2.5 requires numpy<2.0.0,>=1.19.0; python_version >= \"3.9\", but you have numpy 2.0.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed MarkupSafe-3.0.2 certifi-2024.8.30 charset-normalizer-3.4.0 filelock-3.16.1 fsspec-2024.10.0 idna-3.10 jinja2-3.1.4 llvmlite-0.43.0 more-itertools-10.5.0 mpmath-1.3.0 networkx-3.4.2 numba-0.60.0 numpy-2.0.2 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.4.127 openai-whisper-20240930 regex-2024.9.11 requests-2.32.3 sympy-1.13.1 tiktoken-0.8.0 torch-2.5.0 tqdm-4.66.5 triton-3.1.0 typing-extensions-4.12.2 urllib3-2.2.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "certifi",
                  "charset_normalizer",
                  "llvmlite",
                  "numba",
                  "regex",
                  "tiktoken",
                  "tiktoken_ext",
                  "torch",
                  "torchgen",
                  "tqdm",
                  "whisper"
                ]
              },
              "id": "3dc2142555dd45b6b3e627f10dd7f145"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import whisper\n",
        "\n",
        "model = whisper.load_model(\"medium\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "06zmZ7gZMuCK",
        "outputId": "56d62a2d-03e0-4769-ca2f-a788029feb4b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████████████████████████████████| 1.42G/1.42G [00:15<00:00, 98.2MiB/s]\n",
            "/usr/local/lib/python3.10/dist-packages/whisper/__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(fp, map_location=device)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(predicted_text, reference_text):\n",
        "    # Word Error Rate\n",
        "    wer_score = wer(reference_text, predicted_text)\n",
        "\n",
        "    # Character Error Rate\n",
        "    cer_score = cer(reference_text, predicted_text)\n",
        "\n",
        "    # Match Error Rate\n",
        "    mer_score = mer(reference_text, predicted_text)\n",
        "\n",
        "    # Additional metrics (deletion, insertion)\n",
        "    measures = compute_measures(reference_text, predicted_text)\n",
        "    deletion_rate = measures['deletions'] / measures['substitutions']\n",
        "    insertion_rate = measures['insertions'] / measures['substitutions']\n",
        "\n",
        "    # Return all metrics\n",
        "    return {\n",
        "        \"WER\": wer_score,\n",
        "        \"CER\": cer_score,\n",
        "        \"MER\": mer_score,\n",
        "        \"Deletion Rate\": deletion_rate,\n",
        "        \"Insertion Rate\": insertion_rate\n",
        "    }"
      ],
      "metadata": {
        "id": "Gs53lCgPMuEN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import whisper\n",
        "from jiwer import wer, cer, mer, compute_measures\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "import time"
      ],
      "metadata": {
        "id": "gSYRqW4fRHFb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import whisper  # Ensure you have the whisper model imported\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "# Define the paths for audio recordings and transcripts\n",
        "recordings_folder = \"recordings\"\n",
        "transcripts_folder = \"transcribed\"\n",
        "metrics_results = []\n",
        "\n",
        "# Function to compute evaluation metrics\n",
        "def compute_metrics(predicted_text, reference_text):\n",
        "    # Word Error Rate\n",
        "    wer_score = wer(reference_text, predicted_text)\n",
        "\n",
        "    # Character Error Rate\n",
        "    cer_score = cer(reference_text, predicted_text)\n",
        "\n",
        "    # Match Error Rate\n",
        "    mer_score = mer(reference_text, predicted_text)\n",
        "\n",
        "    # Calculate true positives, false positives, and false negatives\n",
        "    reference_words = reference_text.split()\n",
        "    predicted_words = predicted_text.split()\n",
        "\n",
        "    tp = len(set(reference_words) & set(predicted_words))  # True Positives\n",
        "    fp = len(set(predicted_words) - set(reference_words))  # False Positives\n",
        "    fn = len(set(reference_words) - set(predicted_words))  # False Negatives\n",
        "\n",
        "    # Calculate metrics\n",
        "    accuracy = (tp) / (tp + fp + fn) if (tp + fp + fn) > 0 else 0\n",
        "    precision = (tp) / (tp + fp) if (tp + fp) > 0 else 0\n",
        "    recall = (tp) / (tp + fn) if (tp + fn) > 0 else 0\n",
        "    f1 = (2 * precision * recall / (precision + recall)) if (precision + recall) > 0 else 0\n",
        "\n",
        "    # Additional metrics (deletion, insertion)\n",
        "    measures = compute_measures(reference_text, predicted_text)\n",
        "    deletion_rate = measures['deletions'] / measures['substitutions'] if measures['substitutions'] > 0 else 0\n",
        "    insertion_rate = measures['insertions'] / measures['substitutions'] if measures['substitutions'] > 0 else 0\n",
        "\n",
        "    # Return all metrics\n",
        "    return {\n",
        "        \"WER\": wer_score,\n",
        "        \"CER\": cer_score,\n",
        "        \"MER\": mer_score,\n",
        "        \"Accuracy\": accuracy,\n",
        "        \"Precision\": precision,\n",
        "        \"Recall\": recall,\n",
        "        \"F1 Score\": f1,\n",
        "        \"Deletion Rate\": deletion_rate,\n",
        "        \"Insertion Rate\": insertion_rate\n",
        "    }\n",
        "\n",
        "# Loop through each WAV file\n",
        "for filename in os.listdir(recordings_folder):\n",
        "    if filename.endswith(\".wav\"):\n",
        "        wav_path = os.path.join(recordings_folder, filename)\n",
        "\n",
        "        try:\n",
        "            # Load corresponding text transcript\n",
        "            transcript_path = os.path.join(transcripts_folder, filename.replace(\".wav\", \".txt\"))  # Change to .txt\n",
        "            with open(transcript_path, \"r\") as f:\n",
        "                reference_text = f.read().strip()  # Read the text file content\n",
        "\n",
        "            # Start timer for Real-Time Factor\n",
        "            start_time = time.time()\n",
        "\n",
        "            # Transcribe the audio file\n",
        "            result = model.transcribe(wav_path)\n",
        "            predicted_text = result[\"text\"]\n",
        "\n",
        "            # Calculate Real-Time Factor (RTF)\n",
        "            duration = whisper.audio.load_audio(wav_path).shape[-1] / whisper.audio.SAMPLE_RATE\n",
        "            rtf = (time.time() - start_time) / duration\n",
        "\n",
        "            # Compute metrics\n",
        "            metrics = compute_metrics(predicted_text, reference_text)\n",
        "            metrics[\"RTF\"] = rtf  # Add Real-Time Factor\n",
        "            metrics[\"Filename\"] = filename\n",
        "\n",
        "            metrics_results.append(metrics)\n",
        "\n",
        "        except (FileNotFoundError, RuntimeError) as e:\n",
        "            print(f\"Error processing {filename}: {e}\")\n",
        "\n",
        "# Calculate averages\n",
        "if metrics_results:  # Ensure there are results to average\n",
        "    average_metrics = {\n",
        "        \"WER\": sum(result[\"WER\"] for result in metrics_results) / len(metrics_results),\n",
        "        \"CER\": sum(result[\"CER\"] for result in metrics_results) / len(metrics_results),\n",
        "        \"MER\": sum(result[\"MER\"] for result in metrics_results) / len(metrics_results),\n",
        "        \"Accuracy\": sum(result[\"Accuracy\"] for result in metrics_results) / len(metrics_results),\n",
        "        \"Precision\": sum(result[\"Precision\"] for result in metrics_results) / len(metrics_results),\n",
        "        \"Recall\": sum(result[\"Recall\"] for result in metrics_results) / len(metrics_results),\n",
        "        \"F1 Score\": sum(result[\"F1 Score\"] for result in metrics_results) / len(metrics_results),\n",
        "        \"Deletion Rate\": sum(result[\"Deletion Rate\"] for result in metrics_results) / len(metrics_results),\n",
        "        \"Insertion Rate\": sum(result[\"Insertion Rate\"] for result in metrics_results) / len(metrics_results),\n",
        "        \"RTF\": sum(result[\"RTF\"] for result in metrics_results) / len(metrics_results),\n",
        "    }\n",
        "\n",
        "    # Print out average results\n",
        "    print(\"Average Metrics:\")\n",
        "    print(f\"  WER: {average_metrics['WER']:.2f}\")\n",
        "    print(f\"  CER: {average_metrics['CER']:.2f}\")\n",
        "    print(f\"  MER: {average_metrics['MER']:.2f}\")\n",
        "    print(f\"  Accuracy: {average_metrics['Accuracy']:.2f}\")\n",
        "    print(f\"  Precision: {average_metrics['Precision']:.2f}\")\n",
        "    print(f\"  Recall: {average_metrics['Recall']:.2f}\")\n",
        "    print(f\"  F1 Score: {average_metrics['F1 Score']:.2f}\")\n",
        "    print(f\"  Deletion Rate: {average_metrics['Deletion Rate']:.2f}\")\n",
        "    print(f\"  Insertion Rate: {average_metrics['Insertion Rate']:.2f}\")\n",
        "    print(f\"  RTF: {average_metrics['RTF']:.2f}\")\n",
        "else:\n",
        "    print(\"No valid recordings were processed.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TfFUpwHCMuGF",
        "outputId": "863763a9-e73c-49f9-ad25-017ffba46846"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Metrics:\n",
            "  WER: 0.28\n",
            "  CER: 0.18\n",
            "  MER: 0.27\n",
            "  Accuracy: 0.68\n",
            "  Precision: 0.83\n",
            "  Recall: 0.79\n",
            "  F1 Score: 0.81\n",
            "  Deletion Rate: 0.10\n",
            "  Insertion Rate: 0.04\n",
            "  RTF: 0.14\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import whisper\n",
        "\n",
        "# Load the Whisper model\n",
        "model = whisper.load_model(\"medium\")  # You can choose a different model size like 'tiny', 'small', 'medium', or 'large'\n",
        "\n",
        "# Define the path to your audio recordings\n",
        "recordings_folder = \"recordings\"\n",
        "transcripts_folder = \"transcripts\"\n",
        "\n",
        "# Create the transcripts folder if it doesn't exist\n",
        "os.makedirs(transcripts_folder, exist_ok=True)\n",
        "\n",
        "# Loop through each WAV file in the recordings folder\n",
        "for filename in os.listdir(recordings_folder):\n",
        "    if filename.endswith(\".wav\"):\n",
        "        wav_path = os.path.join(recordings_folder, filename)\n",
        "\n",
        "        # Transcribe the audio file\n",
        "        result = model.transcribe(wav_path)\n",
        "        predicted_text = result[\"text\"]\n",
        "\n",
        "        # Save the transcript to a text file\n",
        "        transcript_path = os.path.join(transcripts_folder, filename.replace(\".wav\", \".txt\"))\n",
        "        with open(transcript_path, \"w\") as f:\n",
        "            f.write(predicted_text)\n",
        "\n",
        "        print(f\"Transcribed {filename} and saved to {transcript_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PK8dmotPrICO",
        "outputId": "06e7ae93-3a6d-4b72-99a9-835d4658a63d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/whisper/__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(fp, map_location=device)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transcribed record_out (2).wav and saved to transcripts/record_out (2).txt\n",
            "Transcribed record_out (3).wav and saved to transcripts/record_out (3).txt\n",
            "Transcribed record_out (1).wav and saved to transcripts/record_out (1).txt\n",
            "Transcribed record_out.wav and saved to transcripts/record_out.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import whisper\n",
        "\n",
        "# Load the Whisper model\n",
        "model = whisper.load_model(\"medium\")  # You can choose a different model size like 'tiny', 'small', 'medium', or 'large'\n",
        "\n",
        "# Define the path to your audio recordings\n",
        "recordings_folder = \"recordings\"\n",
        "transcripts_folder = \"transcripts\"\n",
        "\n",
        "# Create the transcripts folder if it doesn't exist\n",
        "os.makedirs(transcripts_folder, exist_ok=True)\n",
        "\n",
        "# Loop through each WAV file in the recordings folder\n",
        "for filename in os.listdir(recordings_folder):\n",
        "    if filename.endswith(\".wav\"):\n",
        "        wav_path = os.path.join(recordings_folder, filename)\n",
        "\n",
        "        # Transcribe the audio file\n",
        "        result = model.transcribe(wav_path)\n",
        "        predicted_text = result[\"text\"]\n",
        "\n",
        "        # Save the transcript to a text file\n",
        "        transcript_path = os.path.join(transcripts_folder, filename.replace(\".wav\", \".txt\"))\n",
        "        with open(transcript_path, \"w\") as f:\n",
        "            f.write(predicted_text)\n",
        "\n",
        "        # Print the transcript to the output\n",
        "        print(f\"Transcribed {filename}:\")\n",
        "        print(predicted_text)\n",
        "        print(\"\\n\" + \"-\" * 40 + \"\\n\")  # Separator for readability\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LIZROieOs9Xf",
        "outputId": "b2e4f9fa-507b-4df2-8691-c3e140229bd8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/whisper/__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(fp, map_location=device)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transcribed record_out (2).wav:\n",
            " Good afternoon, what seems to be the problem? I'm experiencing a lot of fatigue and headaches. How long have you been feeling this way? It's been about 2 weeks now. Let's run some tests to see what might be causing it.\n",
            "\n",
            "----------------------------------------\n",
            "\n",
            "Transcribed record_out (3).wav:\n",
            " Hi there, how can I help you today? I think I might have allergies. I've been sneezing a lot. Have you been exposed to any allergens recently? Yes, I was cleaning out my attic and there was a lot of dust. Let's discuss some allergy treatments.\n",
            "\n",
            "----------------------------------------\n",
            "\n",
            "Transcribed record_out (1).wav:\n",
            " Hello, what brings you in today? I've had this persistent cough for the last week. Have you noticed any other symptoms? Yes, I've also had a bit of a sore throat. Let's take a look and see what's going on.\n",
            "\n",
            "----------------------------------------\n",
            "\n",
            "Transcribed record_out.wav:\n",
            " Good morning, how are you feeling today? I've been feeling really anxious lately. Can you tell me what's been causing your anxiety? I think it is work-related stress I have a lot of on my plate. That's understandable. Let's talk about some coping strategies.\n",
            "\n",
            "----------------------------------------\n",
            "\n"
          ]
        }
      ]
    }
  ]
}